{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel-wise Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelwiseLoss(nn.Module):\n",
    "    def forward(self, inputs, targets):\n",
    "        return F.smooth_l1_loss(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions based on trained models (VGG)\n",
    "Perceptual, Texture, Topological, Content/Style Losses all follow the same principle -\n",
    "1. Extract the feature map's activation output from a trained VGG network for the both the predicted and the target image\n",
    "2. Compare these outputs using a user specified loss function\n",
    "\n",
    "So, we can reuse some of our code across all these loss functions.\n",
    "We define a feature loss class, that takes in -\n",
    "1. loss function, \n",
    "2. VGG block indices from which the feature maps need to be extracted, \n",
    "3. weights to be assigned to the feature map outputs when computing the cumulative loss\n",
    "\n",
    "Note: \n",
    "- You can use any trained CNN network in these loss functions (ResNet, GoogLeNet, VGG etc), but it has been observed that vgg works better when used in this use-case. \n",
    "- In fact adding trained VGG with batch norm performs even better, hence we choose to work with the vgg16_bn from pytorch. \n",
    "- You can also use VGG19 network, which is deeper than VGG16, however it only improves the performance by a slight margin, while adding heavily to the computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd-sata1/hqq/anaconda3/envs/stable_signature/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureHook:\n",
    "    def __init__(self, module):\n",
    "        self.features = None\n",
    "        self.hook = module.register_forward_hook(self.on)\n",
    "\n",
    "    def on(self, module, inputs, outputs):\n",
    "        self.features = outputs\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureLoss(nn.Module):\n",
    "    def __init__(self, loss, blocks, weights, device):\n",
    "        super().__init__()\n",
    "        self.feature_loss = loss\n",
    "        assert all(isinstance(w, (int, float)) for w in weights)\n",
    "        assert len(weights) == len(blocks)\n",
    "\n",
    "        self.weights = torch.tensor(weights).to(device)\n",
    "        #VGG16 contains 5 blocks - 3 convolutions per block and 3 dense layers towards the end\n",
    "        assert len(blocks) <= 5\n",
    "        assert all(i in range(5) for i in blocks)\n",
    "        assert sorted(blocks) == blocks\n",
    "\n",
    "        vgg = vgg16_bn(pretrained=True).features\n",
    "        vgg.eval()\n",
    "\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        vgg = vgg.to(device)\n",
    "\n",
    "        bns = [i - 2 for i, m in enumerate(vgg) if isinstance(m, nn.MaxPool2d)]\n",
    "        assert all(isinstance(vgg[bn], nn.BatchNorm2d) for bn in bns)\n",
    "\n",
    "        self.hooks = [FeatureHook(vgg[bns[i]]) for i in blocks]\n",
    "        self.features = vgg[0: bns[blocks[-1]] + 1]\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "        # normalize foreground pixels to ImageNet statistics for pre-trained VGG\n",
    "        mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        inputs = F.normalize(inputs, mean, std)\n",
    "        targets = F.normalize(targets, mean, std)\n",
    "\n",
    "        # extract feature maps\n",
    "        self.features(inputs)\n",
    "        input_features = [hook.features.clone() for hook in self.hooks]\n",
    "\n",
    "        self.features(targets)\n",
    "        target_features = [hook.features for hook in self.hooks]\n",
    "\n",
    "        loss = 0.0\n",
    "        \n",
    "        # compare their weighted loss\n",
    "        for lhs, rhs, w in zip(input_features, target_features, self.weights):\n",
    "            lhs = lhs.view(lhs.size(0), -1)\n",
    "            rhs = rhs.view(rhs.size(0), -1)\n",
    "            loss += self.feature_loss(lhs, rhs) * w\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptual_loss(x, y):\n",
    "    F.mse_loss(x, y)\n",
    "    \n",
    "def PerceptualLoss(blocks, weights, device):\n",
    "    return FeatureLoss(perceptual_loss, blocks, weights, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m img1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(img0\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 4\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptualLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m, in \u001b[0;36mPerceptualLoss\u001b[0;34m(blocks, weights, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPerceptualLoss\u001b[39m(blocks, weights, device):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFeatureLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperceptual_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mFeatureLoss.__init__\u001b[0;34m(self, loss, blocks, weights, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_loss \u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(w, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weights) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(blocks)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(weights)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "img0 = torch.ones(1,3,64,64)\n",
    "img1 = torch.zeros(1,3,64,64)\n",
    "print(img0.device)\n",
    "loss = PerceptualLoss(img0, img1, img0.device)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texture Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    c, h, w = x.size()\n",
    "    x = x.view(c, -1)\n",
    "    x = torch.mm(x, x.t()) / (c * h * w)\n",
    "    return x\n",
    "\n",
    "def gram_loss(x, y):\n",
    "    return F.mse_loss(gram_matrix(x), gram_matrix(y))\n",
    "\n",
    "def TextureLoss(blocks, weights, device):\n",
    "    return FeatureLoss(gram_loss, blocks, weights, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content/Style Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content, pred):\n",
    "    return FeatureLoss(perceptual_loss, blocks, weights, device)\n",
    "\n",
    "def style_loss(style, pred):\n",
    "    return FeatureLoss(gram_loss, blocks, weights, device)\n",
    "\n",
    "def content_style_loss(content, style, pred, alpha, beta):\n",
    "    return alpha * content_loss(content, pred) + beta * style_loss(style, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topology-aware Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopologyAwareLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, criteria, weights): \n",
    "        # Here criteria -> [PixelwiseLoss, PerceptualLoss], \n",
    "        #weights -> [1, mu] (or any other combination weights)\n",
    "        assert len(weights) == len(criteria)\n",
    "\n",
    "        self.criteria = criteria\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        loss = 0.0\n",
    "        for criterion, w in zip(self.criteria, self.weights):\n",
    "            each = w * criterion(inputs, targets)\n",
    "            loss += each\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN losses\n",
    "Strongly suggest checking out this repository for GAN implementations. https://github.com/eriklindernoren/PyTorch-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxGeneratorLoss(nn.Module):\n",
    "    def forward(self, fake, discriminator):\n",
    "        return torch.log(1 - discriminator(fake))\n",
    "\n",
    "class MinMaxDiscriminatorLoss(nn.Module):\n",
    "    def forward(self, real, fake, discriminator):\n",
    "        return -1.0*(log(discriminator(real)) + log(1-discriminator(fake)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonSaturatingGeneratorLoss(nn.Module):\n",
    "    def forward(self, fake, discriminator):\n",
    "        return -torch.log(discriminator(fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastSquaresGeneratorLoss(nn.Module):\n",
    "    def forward(self, fake, discriminator):\n",
    "        return (discriminator(fake)-1)**2\n",
    "\n",
    "class LeastsquaresDiscriminatorLoss(nn.Module):\n",
    "    def forward(self, real, fake, discriminator):\n",
    "        return (discriminator(real)-1)**2 + discriminator(fake)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wgan has an additional step of clipping the weights between 0, 1\n",
    "#refer - https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan/wgan.py \n",
    "class WGANGeneratorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, fake, discriminator):\n",
    "        return -discriminator(fake).mean()\n",
    "\n",
    "\n",
    "class WGANDiscriminatorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, real, fake, discriminator):\n",
    "        return discriminator(fake).mean() - discriminator(real).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleConsistencyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.loss = nn.L1Loss()\n",
    "    def forward(self, F, G, x, y):\n",
    "        return self.loss(F(G(x)), x) + self.loss(G(F(y)), y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
